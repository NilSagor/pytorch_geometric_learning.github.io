{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nilsagor/opt/anaconda3/bin:/Users/nilsagor/opt/anaconda3/condabin:/opt/homebrew/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "How much features of node \"c\" are important to node \"i\"?\n",
    "\n",
    "Can we learn such importance, in an automatic manner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph  Attention Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input:** a set of node features $h =\\{ \\bar{h_{1}}, \\bar{h_{2}}, \\dots \\bar{h_{n}}\\} \\qquad h_{i} \\in R^F  $\n",
    "\n",
    "Here $n$ is the number of nodes and $F$ is the number of features in each node.\n",
    "\n",
    "**output:** a new set of node features $h^{\\prime} =\\{ \\bar{h_{1}^{\\prime}}, \\bar{h_{2}^{\\prime}} \\dots \\bar{h_{i}^{\\prime}}\\} \\qquad h_{i}^{\\prime} \\in R^F  $\n",
    "\n",
    "initial step, apply a **parameterized linear transformation** to every node \n",
    "\n",
    "$ W \\cdot \\bar{h_{i}} \\qquad W \\in R^{F^{\\prime} \\times F}  $\n",
    "\n",
    "$(F^{\\prime} \\times F) \\cdot F$\n",
    "\n",
    "$F^{\\prime}$\n",
    "\n",
    "**Self attention:**\n",
    "\n",
    "$ a : R^{F^{\\prime}} \\times R^{F^{\\prime}} \\rightarrow R $\n",
    "\n",
    "\n",
    "$e_{i,j} = a(W \\cdot \\bar{h_{i}}, W \\cdot \\bar{h_{j}}) $\n",
    "\n",
    "here $e_{i,j}$ specify the importance of node $j$'s features to node $i$\n",
    "\n",
    "The model allows every node to attend on every other node, dropping all structural information.\n",
    "\n",
    "by inject the graph structure into the mechanism by performing masked attention compute $e_{i,j}$ for nodes $j \\in N_{i}$ wher $N_{i}$ some neighborhood of node $i$ in the graph.\n",
    "\n",
    "\n",
    "**Normalizeation**\n",
    "\n",
    "To make coefficients easily comparable across diffirent nodes, normalize them across all choices of $j$ using the softmax function\n",
    "\n",
    "$a_{i,j} = softmax_{j}(e_{i,j}) = \\frac{exp(e_{i,j})}{\\sum_{k \\in N(i)}exp(e_{i,j})}$\n",
    "\n",
    "$\\alpha$ is a single-layer feed forward neural network, parametrized by a weight vector $a \\in R6{2F^{\\prime}}$ and applying LeakyReLU nonlinearity (with negative input slope $\\alpha = 0.2$). The coefficient computed by the attention mechanism\n",
    "\n",
    "$a_{i,j} = \\frac{exp(LeakyReLU(a^{T}[W\\bar{h_{i}}W\\bar{h_{j}}]))}{\\sum_{k \\in N_{i}}exp(LeakyReLU(a^{T}[W\\bar{h_{i}}W\\bar{h_{j}}]))}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Multi-head attention**\n",
    "\n",
    "Multi-head attention is a module for attention mechanism which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attenttion heads allows for attending to parts of the sequence differently \n",
    "\n",
    "To stabilize the learning process of self-attention, employing multi-head attention would beneficial, Specifically, K independent attention mechanism execute the transformation and then their features are concatenated \n",
    "\n",
    "**Concatention**\n",
    "\n",
    "$h_{i}^{\\prime} = ||_{k=1}^{K} \\sigma(\\sum_{j \\in N(i)} \\alpha_{i,j}{k} W^{k}h_{j})$\n",
    "\n",
    "\n",
    "Here $|| $represents concatenation, $\\alpha_{i,j}^{k}$ are normalized attention coefficient computed by the k-th attention mechanism $W^{k}$ input linear transformation's weight matrix\n",
    "\n",
    "\n",
    "\n",
    "if we perform multi-head attention on the final (prediction) layer of the network, concatenation is no longer sensible, instead employ average and delay applying the final nonlinearity (usually a softmax or logistic sigmoid for classification)\n",
    "\n",
    "**Average**\n",
    "\n",
    "$h_{i}^{\\prime} = \\sigma(\\frac{1}{K}\\sum_{k = 1}^{K} \\sum_{j \\in N(i)} \\alpha_{i,j}{k} W^{k}h_{j})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GATLayer, self).__init__()\n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "in_features = 5\n",
    "out_features = 2\n",
    "nb_nodes = 3\n",
    "\n",
    "W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
    "\n",
    "input = torch.rand(nb_nodes, in_features)\n",
    "\n",
    "h = torch.mm(input, W)\n",
    "N = h.size()[0]\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "nn.init.xavier_uniform_(a.data, gain=1.414)\n",
    "print(a.shape)\n",
    "\n",
    "leakyrelu = nn.LeakyReLU(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_input = torch.cat([h.repeat(1, N).view(N*N, -1), h.repeat(N,1)], dim=1).view(N, -1, 2*out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = leakyrelu(torch.matmul(a_input, a).squeeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4]) torch.Size([4, 1])\n",
      "\n",
      "torch.Size([3, 3, 1])\n",
      "\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(a_input.shape, a.shape)\n",
    "print(\"\")\n",
    "print(torch.matmul(a_input, a).shape)\n",
    "print(\"\")\n",
    "print(torch.matmul(a_input, a).squeeze(2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Masked Attention\n",
    "adj = torch.randint(2, (3,3))\n",
    "zero_vec = -9315*torch.ones_like(e)\n",
    "print(zero_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0]]) \n",
      " tensor([[ 0.8454, -0.0866, -0.0323],\n",
      "        [-0.0710, -0.3267, -0.2724],\n",
      "        [-0.0323, -0.2880, -0.2337]], grad_fn=<LeakyReluBackward0>) \n",
      " tensor([[-9315., -9315., -9315.],\n",
      "        [-9315., -9315., -9315.],\n",
      "        [-9315., -9315., -9315.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.4536e-01, -8.6592e-02, -9.3150e+03],\n",
       "        [-9.3150e+03, -9.3150e+03, -9.3150e+03],\n",
       "        [-9.3150e+03, -2.8795e-01, -9.3150e+03]], grad_fn=<SWhereBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = torch.where(adj>0, e, zero_vec)\n",
    "print(adj, \"\\n\", e, \"\\n\", zero_vec)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = F.softmax(attention, dim=1)\n",
    "h_prime = torch.matmul(attention, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7175, 0.2825, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333],\n",
       "        [0.0000, 1.0000, 0.0000]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0181, 0.6678],\n",
       "        [1.5702, 0.6649],\n",
       "        [1.8825, 0.4424]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0181, 0.6678],\n",
      "        [1.5702, 0.6649],\n",
      "        [1.8825, 0.4424]], grad_fn=<MmBackward>) \n",
      " tensor([[0.6777, 0.7566],\n",
      "        [1.8825, 0.4424],\n",
      "        [2.1503, 0.7958]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(h_prime, \"\\n\", h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the layer\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GATLayer, self).__init__()\n",
    "        \n",
    "    def forward(self):\n",
    "          # Linear transformation\n",
    "            h = torch.mm(input, self.W)\n",
    "            N = h.size()[0]\n",
    "            \n",
    "            #Attention machanism\n",
    "            a_input = torch.cat([h.repeat(1, N).view(N*N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2*self.out_features)\n",
    "            e = self.leakyrelu(torch.matmul(a_input, self.a).sequeeze(2))\n",
    "            \n",
    "            \n",
    "            #masked Attention\n",
    "            zero_vec = -9e15*torch.ones_likes(e)\n",
    "            attention = torch.where(adj > 0, e, zero_vec)\n",
    "            \n",
    "            attention = F.softmax(attention, dim=1)\n",
    "            attention = F.dropout(attention, self.dropout, training = self.training)\n",
    "            h_prime = torch.matmul(attention, h)\n",
    "            \n",
    "            \n",
    "            if self.concat:\n",
    "                return F.relu(h_prime)\n",
    "            else:\n",
    "                return h_prime\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform(self.W.data, gain=1.414)\n",
    "        \n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform(self.a.data, gain=1.414)\n",
    "        \n",
    "        \n",
    "        # leakyRelu\n",
    "        self.leakyrelu = self.LeakyReLU(self.alpha)\n",
    "        \n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "        # linear transformation\n",
    "        h = torch.mm(input, self.W) #matrix multiplication\n",
    "        N = h.size()[0]\n",
    "        print(N)\n",
    "        \n",
    "        # Attention Mechnasim\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N*N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2*self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "        \n",
    "        # Masked Attention\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        \n",
    "        attention = F.softmax(attention, dim = 1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = troch.matmul(attention, h)\n",
    "               \n",
    "             \n",
    "        \n",
    "        if self.concat:\n",
    "            return f.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n",
      "Number of Classes in Cora 7\n",
      "Number of Node Features in Cora 1433\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "name_data = 'Cora'\n",
    "dataset = Planetoid(root='/tmp/'+name_data, name=name_data)\n",
    "dataset.transform = T.NormalizeFeatures()\n",
    "\n",
    "print(f\"Number of Classes in {name_data}\", dataset.num_classes)\n",
    "print(f\"Number of Node Features in {name_data}\", dataset.num_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        self.hid = 8\n",
    "        self.in_head = 8\n",
    "        self.out_head = 1\n",
    "        \n",
    "        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
    "        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat = False, heads =self.out_head, dropout = 0.6)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p = 0.6, training = self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT().to(device)\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay = 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9433, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9428, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9372, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9346, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9334, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9274, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9235, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9233, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9144, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9056, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9022, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8971, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8885, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8814, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8802, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8609, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8534, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8561, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8365, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8349, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8274, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8164, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7880, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7929, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7832, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7803, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7475, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7725, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7313, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7595, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7372, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7242, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7353, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7064, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7037, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6337, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6345, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5811, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6723, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6055, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6285, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6050, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6109, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5555, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5876, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5812, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5305, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5241, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5146, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5149, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5114, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4250, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5782, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4838, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4832, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4296, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5046, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3836, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4526, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4530, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4376, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3597, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4407, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4196, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3384, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3718, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3453, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3845, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2612, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2485, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3211, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2969, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3730, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2754, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3170, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2432, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3316, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2440, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3353, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2256, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2348, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2256, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2132, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2652, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2888, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2490, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1820, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1375, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1819, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1834, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2415, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1688, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1598, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1405, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0916, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1769, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0665, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1955, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0888, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9892, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1202, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1658, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1859, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1198, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0337, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1408, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0471, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0028, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1098, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0934, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0407, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1343, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0110, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9455, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1410, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1034, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1006, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0051, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1150, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0934, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0460, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9492, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0259, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0540, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0096, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0242, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0429, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9571, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0747, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8853, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0067, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0239, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9750, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0489, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9690, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9465, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0433, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9410, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8503, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8776, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8738, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9133, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9493, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9610, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9798, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9487, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8575, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9694, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8932, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9203, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9094, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0111, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8949, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8471, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9633, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8798, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9842, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9052, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8810, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8215, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9874, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0024, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0162, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8965, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9759, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9563, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8597, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9320, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8679, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9592, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8788, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8882, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9478, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9326, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8810, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8128, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8662, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8669, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8560, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9055, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8602, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9147, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8637, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8416, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9047, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7273, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8758, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8124, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9306, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8221, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7886, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9086, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8354, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8562, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8680, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8204, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7579, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8907, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8427, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6655, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8312, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9067, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8689, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8404, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8320, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8323, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7583, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8463, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8718, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7868, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8923, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8128, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7908, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8657, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7793, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7584, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7518, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8708, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9162, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8903, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9090, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8016, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8452, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8051, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8050, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9366, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8648, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8725, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8656, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7480, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7947, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8582, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8817, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7783, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7998, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8457, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7136, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7630, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7938, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7490, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7457, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8169, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8620, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7264, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7270, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7547, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7855, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7997, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8469, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8535, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8038, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7195, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7573, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8281, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8417, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7335, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7338, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7888, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7891, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7531, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7755, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7308, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6968, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7872, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8508, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7587, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7516, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6955, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7836, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7890, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7921, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7258, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6274, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7806, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7564, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8184, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7810, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7273, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7248, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7180, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8287, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7530, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7408, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7672, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7247, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7820, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8324, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8606, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7273, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7868, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7206, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6926, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9087, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7994, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8277, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6707, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7610, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8043, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7905, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7520, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7823, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7067, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7502, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6762, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7256, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6565, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6576, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7152, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7694, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7213, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6667, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7174, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7472, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7013, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6769, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7721, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6898, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7815, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8404, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7153, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7918, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7596, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7139, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6833, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7537, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6554, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7315, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7586, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7439, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7496, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8004, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7611, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7507, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8077, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7481, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7660, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6191, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7366, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7545, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7741, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6902, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6544, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7361, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6820, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7590, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7042, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7931, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7841, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7691, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7022, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6774, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7476, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7249, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7640, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7189, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6588, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7609, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7832, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7445, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7210, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6776, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7419, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7873, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6529, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7562, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7988, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7412, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7637, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7676, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7007, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7133, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7095, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7671, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7345, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7565, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6636, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6493, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6630, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7714, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7875, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7627, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8190, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6243, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6963, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7315, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7862, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6778, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7611, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8037, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7114, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7184, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7419, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7488, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6120, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7013, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6991, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7229, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7364, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7362, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6540, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7583, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7569, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7242, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7244, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6467, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7562, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7615, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7631, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6774, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7908, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7437, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6406, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5855, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6873, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7354, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7030, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6718, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7415, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7360, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7169, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7558, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7167, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7081, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6537, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7169, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6771, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6253, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7185, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7452, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5906, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6559, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7631, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7295, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7151, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6586, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7139, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6627, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5724, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7665, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6809, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6757, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7006, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8089, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6888, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6158, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6843, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7228, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7308, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6873, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6140, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7124, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7323, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7457, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7463, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8078, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6939, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5917, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6598, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7151, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7575, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7341, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8189, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6968, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7419, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6878, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7147, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5968, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7084, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6494, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7039, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7778, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7024, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7754, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7626, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7078, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6683, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7743, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8058, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6805, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7059, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7170, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6609, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7488, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6565, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6319, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6938, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7959, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7618, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6719, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7618, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7761, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7236, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7070, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6325, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7134, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5982, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7269, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6888, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7121, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7062, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5951, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6787, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7047, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6277, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7539, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6960, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6521, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7409, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6677, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6871, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6559, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7157, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6809, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7302, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6163, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5815, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7627, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7547, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6138, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6698, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6495, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6210, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7561, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6882, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7371, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7138, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7005, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6810, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6563, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7164, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6371, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8613, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7658, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6935, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7917, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6026, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6167, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6500, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6299, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7137, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6957, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5810, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7427, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6465, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6528, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6566, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7483, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6533, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7028, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7669, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6485, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7682, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6249, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6926, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6136, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6528, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7169, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7172, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5676, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6970, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7353, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8018, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6688, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6561, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5837, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7053, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7033, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7288, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6817, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6704, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6563, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6953, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5756, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6899, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6494, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7898, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7529, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6128, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6479, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6626, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5412, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6623, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6565, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6646, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6979, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6828, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6955, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7408, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6994, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6469, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6626, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6350, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7499, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7508, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6210, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6742, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6883, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7363, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6332, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7537, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6888, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7653, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6216, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7028, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7812, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7738, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6982, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6249, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6430, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6711, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7214, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7040, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7036, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6602, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6516, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6936, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7212, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6925, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6284, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6336, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6270, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6264, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6269, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7554, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6270, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6116, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6792, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6829, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7228, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6551, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7299, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7307, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5913, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7155, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6886, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5742, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6755, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6829, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6688, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6676, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6454, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7150, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7057, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7248, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7218, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6224, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6522, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7205, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7049, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6870, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8051, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6835, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6780, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7967, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6971, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7066, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6350, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7265, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7306, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6275, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6313, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7547, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5890, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6551, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7505, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7004, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5744, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6212, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6289, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7503, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6723, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6465, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7673, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7035, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7248, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6065, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7118, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6618, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6192, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6694, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7018, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7081, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6959, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6768, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6427, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7470, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6777, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7712, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6966, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6864, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7520, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7241, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5569, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6927, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6788, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5904, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5531, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6057, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6101, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6680, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6242, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7154, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5969, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7139, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7178, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6957, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5876, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6363, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7076, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7176, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6552, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5631, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6507, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7198, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6266, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6798, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7282, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7303, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6422, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6178, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6587, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7087, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6583, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7191, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6094, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6574, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6766, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6896, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7167, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6973, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6351, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6765, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6745, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6855, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7253, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5737, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8137, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6685, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6310, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6052, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6429, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6525, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7105, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6312, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6725, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7311, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6729, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7291, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7890, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5144, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7212, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6534, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7405, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6811, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6536, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7435, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5676, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7285, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7194, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5810, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5841, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7163, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6591, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6602, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6920, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6882, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6573, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6495, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6162, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6337, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6339, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7024, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6336, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6485, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6571, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7323, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7134, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6566, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6231, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5589, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6804, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7041, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6444, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6357, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6336, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5544, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5892, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5824, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6799, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6521, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6022, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6677, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6183, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5911, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5735, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7634, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6538, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6820, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6231, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6108, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7771, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6937, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6837, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6306, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6176, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6559, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5882, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6942, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6022, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6628, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5785, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7238, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6499, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6914, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6190, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6495, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6977, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6106, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6494, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6564, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6936, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6599, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7603, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6529, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6422, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6625, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6654, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5807, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6075, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6612, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6710, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6497, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6681, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5688, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6595, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6809, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6823, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7256, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5620, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7204, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6652, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6177, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6551, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5839, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6971, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6653, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6828, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6301, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6282, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6410, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6718, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6593, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6314, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6506, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5427, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6281, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6817, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6236, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6202, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6443, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7055, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6449, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6436, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6522, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6994, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6674, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6841, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6944, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6874, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7504, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6799, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6115, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7213, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6416, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7299, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6318, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6144, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6892, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7579, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6598, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7082, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6079, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7584, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6463, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5758, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7320, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6906, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6365, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7274, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6813, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7021, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6997, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6336, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6290, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5955, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6597, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6366, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6701, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6070, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7500, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6959, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7323, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6433, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7205, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6925, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6049, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5563, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6345, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8306, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6899, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6458, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5618, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7065, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7794, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7782, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6460, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6341, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6600, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6274, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6706, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6176, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6517, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5881, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6689, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7089, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5946, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5953, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6722, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7479, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7492, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6710, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5565, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7183, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6264, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7522, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5001, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5774, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5697, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7502, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6434, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6775, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6557, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6165, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6918, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6405, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6142, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5827, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6674, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7564, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6223, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6704, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6645, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    \n",
    "    if epoch % 200:\n",
    "        print(loss)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.9000\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / data.test_mask.sum().item()\n",
    "print('Accuracy: {:.4f}'.format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
